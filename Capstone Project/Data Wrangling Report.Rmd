---
title: "Data Wrangling Report"
author: "Aadrika Singh"
date: "September 27, 2017"
output: html_document
---

## Importing dataset

```{r eval=FALSE}
# Reading the dataset ; reading all blanks as NA
telco <- as.data.frame(read.delim("Dataset/orange_small_train.data", na.strings=c("","NA"), header=TRUE, sep="\t", fill=TRUE))

# Add the variable *churn* as a new column
telco$churn <- as.factor(read.table("Dataset/orange_small_train_churn.txt")$V1)
```

There are *50,000* observations of *230* variables in the dataset, from which selected input features will be used to predict the outcome variable **churn**. From the dataset description, we know that the first *190* variables are **numerical** and the last *40* are **categorical**.

## Cleaning the dataset

As observed, there are a lot of *missing values* in the dataset. To tackle this situation, I'll look at the proportion of missing values in the columns.

```{r eval=FALSE}
library(ggplot2)

# Looking at the proportion of missing values in the remaining columns
checkNA <- sapply(X = telco, FUN = function(x){sum(is.na(x))/nrow(telco)})

# Plotting a histogram of the proportion of missing values
hist(checkNA)
```

As observed from the histogram, I'll remove the ones where more than 20% of the data is missing (some of the columns have 100% missing data, and otherwise, there are not many variables with more than 20% missing values), as these variables will not contribute much towards the final goal of prediction.

```{r eval=FALSE}
# Removing the columns which have more than 20% NA values
telco <- telco[, checkNA <= 0.2]
```

The dataset now has only *67* variables, all of which have **less than 20%** missing data.

For categorical variables, all the variables with more than 50 factor levels have been removed.

```{r eval=FALSE}
for(i in colnames(telco)[39:66]){
  if(length(levels(telco[[i]])) > 50) {
   telco[[i]] <- NULL
   }
 }
```

Removing variable with near-zero variance, as they may not be useful for discriminating classes.

```{r eval=FALSE}
library(caret)

# Get indices of columns with low variance
badCols <- nearZeroVar(telco)
print(paste("Fraction of nearZeroVar columns:", round(length(badCols)/length(telco),4)))

# Remove columns from dataset
telco <- telco[, -badCols]
```

The dataset now has *45* variables.

```{r eval=FALSE}
# Replacing the column names to be in proper order
names(telco)[1:44] <- paste("V", 1:44, sep = "")

# Replacing -1 with 0 for churn variable
levels(telco$churn)[telco$churn == "-1"] <- "0"

# Relabel the churn variable
telco$churn <- factor(telco$churn, levels=c('0', '1'),
  labels=c('No', 'Yes'))
```

## Dealing with missing data

I am using **Hmisc** package for imputing the missing values for numerical variables. I've excluded the variables that have no missing values, or are heavily skewed, to facilitate the imputation to run. For the variables that are excluded from the previous step, I'm using median imputation to fill in the missing values. 

For categorical variables, all the NA values will be replaced by another factor level called "Unknown".

```{r eval=FALSE}
library(Hmisc)
set.seed(144)

# Identify the variables that have no missing values, to remove them from the imputation
checkNA <- sapply(X = telco, FUN = function(x){sum(is.na(x))/nrow(telco)})
names(telco[, checkNA == 0])

# Analyze summary for numerical variables
summary(subset(telco, select = colnames(telco)[1:11]))
summary(subset(telco, select = colnames(telco)[12:22]))
summary(subset(telco, select = colnames(telco)[23:32]))

# Use imputation 
f <- aregImpute(~V1+V2+V3+V4+V5+V6+V7+V8+V10+V14+V15+V16+V17+V18+V19+V20+V22+V23+V24+V26+V27+V28+V29+V30+V31+V32, data = telco, n.impute = 5)
# Get the imputed values
imputed <-impute.transcan(f, data=telco, imputation=1, list.out=TRUE, pr=FALSE, check=FALSE)
# convert the list to the database
imputed.data <- as.data.frame(do.call(cbind,imputed))
# arrange the columns accordingly
imputed.data <- imputed.data[, colnames(telco)[c(1,2,3,4,5,6,7,8,10,14,15,16,17,18,19,20,22,23,24,26,27,28,29,30,31,32)], drop = FALSE]
# update the dataset
for(i in colnames(telco)[c(1,2,3,4,5,6,7,8,10,14,15,16,17,18,19,20,22,23,24,26,27,28,29,30,31,32)]){
 telco[[i]] <- imputed.data[[i]]
}

# Dealing with missing values for categorical variables
for(i in colnames(telco)[33:44]){
 levels(telco[[i]]) <- c(levels(telco[[i]]), "Unknown") 
 telco[[i]][is.na(telco[[i]])] <- "Unknown"
}

# Drop unused levels
for(i in colnames(telco)[33:44]){
 telco[[i]] <- droplevels(telco[[i]])
}

# Identify the variables that have missing values, to fill the missing values
checkNA <- sapply(X = telco, FUN = function(x){sum(is.na(x))/nrow(telco)})
names(telco[, checkNA != 0])

# Function to replace NA values with median of the column
replaceMed <- function(x){ifelse(is.na(x), median(x, na.rm = TRUE), x)}

# Replacing the NA value in the columns with median of the column
for(i in colnames(telco)[c(9,12,25)]){
 telco[[i]] <- replaceMed(telco[[i]])
}

# Ensuring that no variable has a missing value anymore
checkNA <- sapply(X = telco, FUN = function(x){sum(is.na(x))/nrow(telco)})
names(telco[, checkNA != 0])

# Exporting the clean data set to a CSV file
write.csv(x = telco, file = "telcoClean.csv")
