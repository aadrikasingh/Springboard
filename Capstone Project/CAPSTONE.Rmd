---
title: "CAPSTONE PROJECT"
author: "Aadrika Singh"
date: "September 27, 2017"
output:
  pdf_document: default
  html_document: default
---

## Importing dataset

```{r results='hide'}
# Reading the dataset ; reading all blanks as NA
telco <- as.data.frame(read.delim("Dataset/orange_small_train.data", na.strings=c("","NA"), header=TRUE, sep="\t", fill=TRUE))

# Add the variable *churn* as a new column
telco$churn <- as.factor(read.table("Dataset/orange_small_train_churn.txt")$V1)
```

```{r}
dim(telco)
```

There are *50,000* observations of *230* variables in the dataset, from which selected input features will be used to predict the outcome or 231st variable **churn**. From the dataset description, we know that the first *190* variables are **numerical** and the rest of the *40* are **categorical**.

## Cleaning the dataset

As observed, there are a lot of *missing values* in the dataset. To tackle this situation, I'll look at the proportion of missing values in the columns.

```{r results='hide'}
library(ggplot2)

# Looking at the proportion of missing values in the remaining columns
checkNA <- sapply(X = telco, FUN = function(x){sum(is.na(x))/nrow(telco)})
```

```{r}
# Plotting a histogram of the proportion of missing values
hist(checkNA)
```

As observed from the histogram, I'll remove the columns where more than 20% of the data is missing (some of the columns have 100% missing data, and otherwise, there are not many variables with more than 20% missing values), as these variables will not contribute much towards the final goal of prediction.

```{r results='hide'}
# Removing the columns which have more than 20% NA values
telco <- telco[, checkNA <= 0.2]
```

```{r}
dim(telco)
```

The dataset now has only *67* variables, all of which have **less than 20%** missing data.

Removing variable with near-zero variance, as they may not be useful for discriminating classes.

```{r results='hide'}
library(caret)

# Get indices of columns with low variance
badCols <- nearZeroVar(telco)
```

```{r}
print(paste("Fraction of nearZeroVar columns:", round(length(badCols)/length(telco),4)))
```

```{r results='hide'}
# Remove columns from dataset
telco <- telco[, -badCols]
```

```{r}
dim(telco)
```

The dataset now has *57* variables.

```{r results='hide'}
# Replacing the column names to be in proper order
names(telco)[1:56] <- paste("V", 1:56, sep = "")

# Replacing -1 with 0 for churn variable
levels(telco$churn)[telco$churn == "-1"] <- "0"

# Relabel the churn variable
telco$churn <- factor(telco$churn, levels=c('0', '1'),
  labels=c('No', 'Yes'))
```

## Dealing with missing data

I am using **Hmisc** package for imputing the missing values for numerical variables. I've excluded the variables that have no missing values, or are heavily skewed, to facilitate the imputation to run. For the variables that are excluded from the previous step, I'm using median imputation to fill in the missing values. 

For categorical variables, all the NA values will be replaced by another factor level called "Unknown".

```{r results='hide'}
library(Hmisc)
set.seed(144)

# Identify the variables that have no missing values, to remove them from the imputation
checkNA <- sapply(X = telco, FUN = function(x){sum(is.na(x))/nrow(telco)})
```

```{r}
names(telco[, checkNA == 0])
```

```{r results='hide'}
# Analyze summary for numerical variables
summary(subset(telco, select = colnames(telco)[1:11]))
summary(subset(telco, select = colnames(telco)[12:22]))
summary(subset(telco, select = colnames(telco)[23:32]))

# Use imputation 
f <- aregImpute(~V1+V2+V3+V4+V5+V6+V7+V8+V10+V14+V15+V16+V17+V18+V19+V20+V22+V23+V24+V26+V27+V28+V29+V30+V31+V32, data = telco, n.impute = 5)
# Get the imputed values
imputed <-impute.transcan(f, data=telco, imputation=1, list.out=TRUE, pr=FALSE, check=FALSE)
# Convert the list to the database
imputed.data <- as.data.frame(do.call(cbind,imputed))
# Arrange the columns accordingly
imputed.data <- imputed.data[, colnames(telco)[c(1,2,3,4,5,6,7,8,10,14,15,16,17,18,19,20,22,23,24,26,27,28,29,30,31,32)], drop = FALSE]
# Update the dataset
for(i in colnames(telco)[c(1,2,3,4,5,6,7,8,10,14,15,16,17,18,19,20,22,23,24,26,27,28,29,30,31,32)]){
 telco[[i]] <- imputed.data[[i]]
}

# Dealing with missing values for categorical variables - adding the level "Unknown"
for(i in colnames(telco)[33:56]){
 levels(telco[[i]]) <- c(levels(telco[[i]]), "Unknown") 
 telco[[i]][is.na(telco[[i]])] <- "Unknown"
}

# Drop unused levels
for(i in colnames(telco)[33:56]){
 telco[[i]] <- droplevels(telco[[i]])
}

# Identify the variables that have missing values, to fill the missing values
checkNA <- sapply(X = telco, FUN = function(x){sum(is.na(x))/nrow(telco)})
```

```{r}
names(telco[, checkNA != 0])
```

```{r results='hide'}
# Function to replace NA values with median of the column
replaceMed <- function(x){ifelse(is.na(x), median(x, na.rm = TRUE), x)}

# Replacing the NA value in the columns with median of the column
for(i in colnames(telco)[c(9,12,25)]){
 telco[[i]] <- replaceMed(telco[[i]])
}

# Ensuring that no variable has a missing value anymore
checkNA <- sapply(X = telco, FUN = function(x){sum(is.na(x))/nrow(telco)})
```

```{r}
names(telco[, checkNA != 0])
```

```{r results='hide'}
# Exporting the clean data set to a CSV file
write.csv(x = telco, file = "telcoClean.csv")
```

## Machine Learning

```{r results='hide'}
# Splitting the dataset into training and test sets
library(caTools)
split <- sample.split(telco$churn, SplitRatio = 0.75)
train <- subset(telco, split == TRUE)
test <- subset(telco, split == FALSE)
```

```{r results='hide'}
########### Random Forests ###########
library(randomForest)
library(plyr)

# Since randomForest can't handle categorical predictors with more than 53 categories :
trainRF <- train
for(i in colnames(trainRF)[33:56]){
  if(length(levels(trainRF[[i]])) > 50) {
   trainRF[[i]] <- NULL
   }
}

testRF <- test
for(i in colnames(testRF)[33:56]){
  if(length(levels(testRF[[i]])) > 50) {
   testRF[[i]] <- NULL
   }
 }

# Fit the random Forest model using all variables
RFmod <- randomForest(factor(churn)~.,data = trainRF)

# Confusion matrix for prediction 
table(predict(object = RFmod, test), test$churn)
```

```{r}
# Use varImpPlot to select top 10 variables
varImpPlot(RFmod)
```

```{r results='hide'}
# Top 10 variables : V54, V21, V11, V16, V1, V30, V8, V26, V22, V27
# Remove the trainRF and testRF datasets
rm(trainRF)
rm(testRF)

# Set up parallel computation
library(doParallel)
cl <- makeCluster(2)
registerDoParallel(cl)

# Specify the type of resampling
fitControl <- trainControl(method = "repeatedcv",
                           number = 5,
                           repeats = 3,
                           summaryFunction = prSummary,
                           classProbs = TRUE,
                           allowParallel = TRUE)

set.seed(825)

# Fit the random forest model with the selected 10 variables
rfFit1 <- train(train[,c("V54", "V21", "V11", "V16", "V1", "V30", "V8", "V26", "V22", "V27")],
                train[,"churn"],
                data = train,
                method = "rf",
                trControl = fitControl,
                metric = "F"
                )

rfFit1

# Confusion matrix
confusionMatrix(predict.train(rfFit1, test, type = "raw"), test$churn)

# Stop parallel computation
stopCluster(cl)
registerDoSEQ()

```


```{r results='hide'}
########### LASSO ###########
# lassoFit2 <- train(train[,-"churn"], train[,"churn"],
#                 data = train,
#                 method = "glmnet",
#                 trControl = fitControl,
#                 metric = "F",
#                 tuneGrid=expand.grid(
#                   .alpha=1,
#                   .lambda=seq(0, 100, by = 0.1))
#                 )

########### FOBA ###########
library(foba)
```
